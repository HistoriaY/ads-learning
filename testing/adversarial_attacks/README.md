# Adversarial Attacks

Adversarial attacks in autonomous driving refer to attempts to fool or manipulate the systems of autonomous vehicles (AVs) using carefully crafted inputs known as adversarial examples. These examples are usually formed by introducing small perturbations to normal inputs (like images or sensor data) which lead the AV's machine learning models to make incorrect predictions or interpretations.

The potential for adversarial attacks on AVs highlights the need for robustness in the design of autonomous systems. Research in this field focuses on understanding the nature of these attacks, their implications, and developing methods to mitigate their effects. Techniques like adversarial training (where the model is trained with adversarial examples), defensive distillation, and feature squeezing are among the methods used to increase the robustness of machine learning models against adversarial attacks. It's an active area of research given the high-stakes nature of autonomous driving and the need to ensure safety and reliability under all conditions.


## Table of Contents
* [Sensor Spoofing](sensor_spoofing.md)
* [Physical-World Attacks](phys_attacks.md)



---

For more works, you can refer to [Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses](https://arxiv.org/pdf/2104.01789).

